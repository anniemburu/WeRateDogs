{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRate Dogs Wrangling Report\n",
    "\n",
    "## Introduction\n",
    "The aim of this project is to test Data Wrangling Skills and Techniques. For this project, I wrangled, analysed and visualize a tweet archive belonging to the twitter account [@dog_rates](\"https://twitter.com/dog_rates\") also known as WeRateDogs. This twitter account rates people's dogs with humorous comments. \n",
    "\n",
    "## Data Wrangling\n",
    "I first gathered the 3 pieces of dataset I required as follows:\n",
    "- The WeRate Twitter archive which I downloaded manually from the Udacity servers which I saved as **twitter_archive_enhanced.csv**.\n",
    "- The tweet image prediction dataset which I programatically downloaded from Udacity servers and saved as **image_predictions.tsv**.\n",
    "- The tweets' entire set of JSON data which I downloaded from Twitter's API using the python library, tweepy and saved it as **tweet_json.txt**.\n",
    "\n",
    "I then loaded these files in dataframes using pandas as **twitter_archive_df**, **image_predictions_df** and **tweet_json_df** respectively.\n",
    "\n",
    "## Data Assessment\n",
    "I began the assessment visually by looking though the files in a spreadsheet, in order to get the genreal idea of the content of the dataset and to spot any errors I can. I then then programatically on jupyter notebook to get the detailed structure of the datasets as well as programatically dish out errors. I checked for duplicates, null values, incorrect entries, incorrect data types for the variables and any other anomalies in the content and structure in the datasets as these are the issued which bring about dirty and messy data. At the end of assessment I had found several issues with the datasets which I noted down to use in cleaning the data.\n",
    "\n",
    "## Cleaning\n",
    "In the data cleaning stage I started by first making a copy of each of the 3 datasets as:\n",
    "- twitter_archive_clean\n",
    "- image_predictions_clean\n",
    "- tweet_json_clean\n",
    "\n",
    "I first tackling **twitter_archive_clean** dataframe:\n",
    "I began by dropping all the retweets from the dataset as I was only interested in the original tweets. I therefore used `retweeted_status_user_id` to eliminate all the rows that are retweets.\n",
    "\n",
    "I then dropped all the columns that I did not need for analysis after the cleaning process. I dropped the following columns from the columns: `in_reply_to_status_id`, `in_reply_to_user_id`, `source`, `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp`, `expanded_urls`, and `name`. The reason I dropped columns first before dropping rows with null values is to avoid dropping rows that had null values in the rows which I will not use for analysis.\n",
    "\n",
    "I followed by changing the data type of variables that has incorrects one as follows:\n",
    "- `timestamp` changed to datetime.\n",
    "- `rating_numerator` changed to float.\n",
    "\n",
    "Next, I addressed the numerous  missing incorrect values of `rating_numerator` and `rating_denominator`. The issues with this columns were:\n",
    "- Some values for both columns were large. In such case the tweet account rated a picture with several dogs hence the large ratings.\n",
    "- Some tweets had no ratings.\n",
    "- Some ratings were just wrong as compared to the original tweet.\n",
    "I therefore manually cleaned these errors manually with the correct values and dropped the rows that did not have any rating.\n",
    "\n",
    "Lastly, I combined the `doggo`,`floffer`,`pupper` and `puppo` to one column which which I named `dog_stage`. I changed it's datatype to categorical datatype. Afterwards I realized it has wrong information as it had a lot of missing data so I dropped the column entirely.\n",
    "\n",
    "I then tackled **image_predictions_clean** dataframe:\n",
    "For this dataframe, I started by dropping all the rows that did not have any prediction. This means that if all `p1_dog`,`p2_dog` and `p3_dog` were false, that entry had no prediction, I therefore dropped them.\n",
    "\n",
    "I then choose one dog breed among the three possible dog_breeds `p1`,`p2` and `p3` by using it's corresponding cofidence `p1_conf`,`p2_conf` and `p3_conf`. I choose the highest of the three confidences with a valid dog_breed. I saved the newly acquired dog breeds in a new column named `dog_breed`. I then changed its data type to categorical data type.\n",
    "\n",
    "I lastly dropped all the unnecessary columns from the dataset which are `p1`, `p2`, `p3`, `p1_conf`, `p2_conf`, `p3_conf`, `p1_dog`, `p2_dog`, `p3_dog`.\n",
    "\n",
    "I lastly cleaned **tweet_json_clean** dataframe:\n",
    "For this last dataset, I started by changing incorrect data types as:\n",
    "- `tweet_id` changed to int datatype.\n",
    "- `date_time` changed to datetime datatype.\n",
    "\n",
    "I lastly changed the column name of the `date_time` column to *date_time* so that it will be easy when I will be joing the datasets.\n",
    "\n",
    "\n",
    "After cleaning all the datasets individually, I joined all the 3 tables to one which I saved as **rate_dogs_df**. I then dropped all the rows that had null values. I lastly saved the cleaned dataset as a csv file named; **twitter_archive_master.csv**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
