{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Data Wrangling Project\n",
    "The dataset being used is a twitter archive of the Twitter user @dog_rates also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dogs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#gather\">Gathering</a></li>\n",
    "<li><a href=\"#asses\">Assessing</a></li>\n",
    "<li><a href=\"#quality\">Quality</a></li>\n",
    "<li><a href=\"#tidiness\">Tidiness</a></li>\n",
    "<li><a href=\"#clean\">Cleaning</a></li>\n",
    "<li><a href=\"#analyse\">Analysing and Vizualization</a></li>\n",
    "<li><a href=\"#insight\">Insights</a></li>\n",
    "<li><a href=\"#ref\">Reference</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d00e69ec285e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import tweepy\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "## Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load archive data\n",
    "twitter_archive_df = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "twitter_archive_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the image prediction data\n",
    "url = ' https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "r = requests.get(url)\n",
    "\n",
    "#Check the kind of data being fetched\n",
    "print(r.headers.get('content-type'))\n",
    "\n",
    "#Save the file\n",
    "open('image_predictions.tsv', 'wb').write(r.content)\n",
    "\n",
    "# read the .tsv in a datframe\n",
    "image_predictions_df = pd.read_csv('image_predictions.tsv' ,sep='\\t')\n",
    "image_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test tweepy\n",
    "\n",
    "#auth = tweepy.OAuthHandler(\"CONSUMER_KEY\", \"CONSUMER_SECRET\")\n",
    "#auth.set_access_token(\"ACCESS_TOKEN\", \"ACCESS_TOKEN_SECRET\")\n",
    "\n",
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(\"JZm9suzrkWGH8q9LDXdRvQTRf\",\"KUEYw3ezHz1ZiWv1b8TsLKg6moGW1YagWR5An08V6a5gkD0TLx\")\n",
    "auth.set_access_token(\"1676399646-yO1VMF02zCZbsORZCHRzbPqrFfdFcUHTIkY7nzY\", \"3jwmwYvLWO8FSBkJhmXuDNn3ICyrMOHvCZjn5uDUkThWH\")\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tweepy to extract retweets and likes\n",
    "#**SIDE NOTE: DEL API KEYS B4 SUBMISSION**\n",
    "    \n",
    "# Authenticate to Twitter\n",
    "CONSUMER_KEY = \"JZm9suzrkWGH8q9LDXdRvQTRf\"\n",
    "CONSUMER_SECRET = \"KUEYw3ezHz1ZiWv1b8TsLKg6moGW1YagWR5An08V6a5gkD0TLx\"\n",
    "ACCESS_TOKEN = \"1676399646-yO1VMF02zCZbsORZCHRzbPqrFfdFcUHTIkY7nzY\"\n",
    "ACCESS_TOKEN_SECRET = \"3jwmwYvLWO8FSBkJhmXuDNn3ICyrMOHvCZjn5uDUkThWH\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# Create API object\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ids = twitter_archive_df.tweet_id.values\n",
    "tweet_ids = np.array(tweet_ids)\n",
    "len(tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "fails_dict = {}\n",
    "start = time.time()\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open('tweet_json.txt', 'w', encoding='utf8') as file:\n",
    "    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "    for tweet_id in tweet_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode= 'extended')._json\n",
    "            print(\"Success\")\n",
    "            json.dump(tweet, file)\n",
    "            file.write('\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Fail\")\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = time.time()\n",
    "print(fails_dict)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_list = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "try:\n",
    "    with open('tweet_json.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            \n",
    "            #tweet id\n",
    "            tweet_id = tweet['id_str']\n",
    "            \n",
    "            # number of likes in the tweet\n",
    "            favorites = tweet['favorite_count']\n",
    "\n",
    "            # number of retweets for the tweet\n",
    "            retweets = tweet['retweet_count'] \n",
    "\n",
    "            #tweet's timestamp\n",
    "            date_time = tweet['created_at']\n",
    "            \n",
    "            # append the fields\n",
    "            tweet_json_list. append({'tweet_id': tweet_id,\n",
    "                                    'favorites': favorites,\n",
    "                                    'retweets': retweets,\n",
    "                                    'date_time': date_time})\n",
    "except FileNotFoundError:\n",
    "    print(\"Oops! No such file\")\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_df = pd.DataFrame(tweet_json_list, columns=['tweet_id', 'favorites', 'retweets','date_time'])\n",
    "tweet_json_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='asses'></a>\n",
    "## Assessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Assess `twitter_archive_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_df['rating_numerator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_df['rating_denominator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "twitter_archive_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view basic statistical details\n",
    "twitter_archive_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='quality'></a>\n",
    "#### Quality\n",
    "**`twitter_archive_df` table**\n",
    "- dataframe contains retweets.\n",
    "- missing data in columns `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp` and `expanded_urls`.\n",
    "- Incorrect datatype(`timestamp`,`retweeted_status_timestamp`,`in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp`, `rating_numerator` columns).\n",
    "- Some entries have `rating_denominator` column has denominator more than 10.\n",
    "- Some enties in both `rating_numerators` and `rating_denominator` are very large.\n",
    "- Some entries in the `rating_numerator` and `rating_denominator` column are different from the extracted text\n",
    "- Some rows are missing ratings.\n",
    "\n",
    "\n",
    "**`image_prediction_df` table**\n",
    "- missing entries - Has less enties than `twitter_archive_df` table.\n",
    "- incorrect datatypes(p1,p2 and p3 columns).\n",
    "- some entries do not have a dog breed.\n",
    "\n",
    "**`tweet_json_df` table**\n",
    "- missing entries - Has less enties than `twitter_archive_df` table.\n",
    "- Incorrect datatype (`date_time` and `tweet_id` columns).\n",
    "\n",
    "<a id='tidiness'></a>\n",
    "#### Tidiness\n",
    "- *date_time* column from `tweet_json_df` and *timestamp* column from `twitter_archive_df` have same content but diff column names.\n",
    "- The dog stages, *doggo*, *floofer*, *pupper*, *puppo* are in four different columns in `twitter_archive_df` table.\n",
    "- Some columns are not needed for analysis in all the 3 datasets.\n",
    "- All the three tables belong to one table.\n",
    "- Dataframe `image_prediction_df` has 3 different columns with possible dog breeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean = twitter_archive_df.copy()\n",
    "image_predictions_clean = image_predictions_df.copy()\n",
    "tweet_json_clean = tweet_json_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `twitter_archive_clean` Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "\n",
    "- Use `retweeted_status_id` to delete retweets. Any row with content in this column or `retweeted_status_user_id` is a retweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete retweets\n",
    "twitter_archive_clean = twitter_archive_clean[pd.isnull(twitter_archive_clean['retweeted_status_user_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Drop all the unnccessary columns from the table ('in_reply_to_status_id', 'in_reply_to_user_id', 'source','retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp', 'expanded_urls). This will also take care of the missing data in those columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean.drop(['in_reply_to_status_id', 'in_reply_to_user_id', 'source','retweeted_status_id',\n",
    "                           'retweeted_status_user_id','retweeted_status_timestamp', 'expanded_urls','name'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Change `timestamp` to datetime and `rating_numerator` to float data types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datatype to datetime\n",
    "twitter_archive_clean['timestamp'] = pd.to_datetime(twitter_archive_clean['timestamp'])\n",
    "twitter_archive_clean['rating_numerator'] = twitter_archive_clean['rating_numerator'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Correct all the incorrect numerators to match the value in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the incorrect whole\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 666287406224695296) & (twitter_archive_clean['rating_numerator'] == 1), ['rating_numerator']] = 9\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 881633300179243008) & (twitter_archive_clean['rating_numerator'] == 17), ['rating_numerator']] = 13\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 835246439529840640) & (twitter_archive_clean['rating_numerator'] == 960), ['rating_numerator']] = 13\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 716439118184652801) & (twitter_archive_clean['rating_numerator'] == 50), ['rating_numerator']] = 11\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 722974582966214656) & (twitter_archive_clean['rating_numerator'] == 4), ['rating_numerator']] = 13\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 682962037429899265) & (twitter_archive_clean['rating_numerator'] == 7), ['rating_numerator']] = 10\n",
    "\n",
    "#change incorrect floats\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 786709082849828864) & (twitter_archive_clean['rating_numerator'] == 75), ['rating_numerator']] = 9.75\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 778027034220126208) & (twitter_archive_clean['rating_numerator'] == 27), ['rating_numerator']] = 11.27\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 680494726643068929) & (twitter_archive_clean['rating_numerator'] == 26), ['rating_numerator']] = 11.26\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 883482846933004288) & (twitter_archive_clean['rating_numerator'] == 5), ['rating_numerator']] = 13.5\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 681340665377193984) & (twitter_archive_clean['rating_numerator'] == 5), ['rating_numerator']] = 9.5\n",
    "\n",
    "\n",
    "# one pic many dogs\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 731156023742988288) & (twitter_archive_clean['rating_numerator'] == 204), ['rating_numerator']] = 12\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 758467244762497024) & (twitter_archive_clean['rating_numerator'] == 165), ['rating_numerator']] = 11\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 677716515794329600) & (twitter_archive_clean['rating_numerator'] == 144), ['rating_numerator']] = 12\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 684225744407494656) & (twitter_archive_clean['rating_numerator'] == 143), ['rating_numerator']] = 11\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 684222868335505415) & (twitter_archive_clean['rating_numerator'] == 121), ['rating_numerator']] = 11\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 713900603437621249) & (twitter_archive_clean['rating_numerator'] == 99), ['rating_numerator']] = 11\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 675853064436391936) & (twitter_archive_clean['rating_numerator'] == 88), ['rating_numerator']] = 11\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 820690176645140481) & (twitter_archive_clean['rating_numerator'] == 84), ['rating_numerator']] = 12\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 710658690886586372) & (twitter_archive_clean['rating_numerator'] == 80), ['rating_numerator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 704054845121142784) & (twitter_archive_clean['rating_numerator'] == 60), ['rating_numerator']] = 12\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 709198395643068416) & (twitter_archive_clean['rating_numerator'] == 45), ['rating_numerator']] = 9\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 697463031882764288) & (twitter_archive_clean['rating_numerator'] == 44), ['rating_numerator']] = 11\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 686035780142297088) & (twitter_archive_clean['rating_numerator'] == 10), ['rating_numerator']] = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean['rating_numerator'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Change the `rating_denominator` of incorrect all entries to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change incorrect denoms\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 666287406224695296) & (twitter_archive_clean['rating_denominator'] == 2), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 681340665377193984) & (twitter_archive_clean['rating_denominator'] == 5), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 740373189193256964) & (twitter_archive_clean['rating_denominator'] == 11), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 716439118184652801) & (twitter_archive_clean['rating_denominator'] == 50), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 722974582966214656) & (twitter_archive_clean['rating_denominator'] == 20), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 682962037429899265) & (twitter_archive_clean['rating_denominator'] == 11), ['rating_denominator']] = 10\n",
    "\n",
    "\n",
    "#the one pic many dog denom\n",
    "\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 731156023742988288) & (twitter_archive_clean['rating_denominator'] == 170), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 758467244762497024) & (twitter_archive_clean['rating_denominator'] == 150), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 677716515794329600) & (twitter_archive_clean['rating_denominator'] == 120), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 684225744407494656) & (twitter_archive_clean['rating_denominator'] == 130), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 684222868335505415) & (twitter_archive_clean['rating_denominator'] == 110), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 713900603437621249) & (twitter_archive_clean['rating_denominator'] == 90), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 675853064436391936) & (twitter_archive_clean['rating_denominator'] == 80), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 820690176645140481) & (twitter_archive_clean['rating_denominator'] == 70), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 710658690886586372) & (twitter_archive_clean['rating_denominator'] == 80), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 704054845121142784) & (twitter_archive_clean['rating_denominator'] == 50), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 709198395643068416) & (twitter_archive_clean['rating_denominator'] == 50), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 697463031882764288) & (twitter_archive_clean['rating_denominator'] == 40), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 835246439529840640) & (twitter_archive_clean['rating_denominator'] == 0), ['rating_denominator']] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean['tweet_id'] == 686035780142297088) & (twitter_archive_clean['rating_denominator'] == 20), ['rating_denominator']] = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean['rating_denominator'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drop the rows with ratings; 20/16, 24/7, 11/15 as they have no proper ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with no clear ratings\n",
    "twitter_archive_clean.drop([342,516,1663], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean['rating_denominator'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define TO BE LOOKED AT\n",
    "- Melt the *'doggo'*, *'floofer'*, *'pupper'* and *'puppo'* columns into one column called *dog_stage*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melt the columns and creat one column\n",
    "twitter_archive_clean = pd.melt(twitter_archive_clean, id_vars=['tweet_id', 'timestamp', 'text', 'rating_numerator',\n",
    "                                                               'rating_denominator'],\n",
    "                               value_vars = ['doggo', 'floofer', 'pupper', 'puppo'], var_name = 'dog_stage')\n",
    "\n",
    "#drop the value column\n",
    "twitter_archive_clean = twitter_archive_clean.drop('value', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean['dog_stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_stage_classes = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "pd_ver = pd.__version__.split(\".\")\n",
    "if (int(pd_ver[0]) > 0) or (int(pd_ver[1]) >= 21): # v0.21 or later\n",
    "    dclasses = pd.api.types.CategoricalDtype(ordered = False, categories = dog_stage_classes)\n",
    "    twitter_archive_clean['dog_stage'] = twitter_archive_clean['dog_stage'].astype(dclasses)\n",
    "else: # pre-v0.21\n",
    "    twitter_archive_clean['dog_stage'] = twitter_archive_clean['dog_stage'].astype('category', ordered = False,\n",
    "                                                         categories = dog_stage_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Drop the dog_stage column as the data is in correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean.drop(['dog_stage'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `image_predictions_df` Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Delete the entries where there is no dog breed prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with no dog breed prediction\n",
    "wrong_breeds = image_predictions_clean[~image_predictions_clean.p1_dog & \\\n",
    "               ~image_predictions_clean.p2_dog & \\\n",
    "               ~image_predictions_clean.p3_dog][['tweet_id', 'p1', 'p1_dog',\n",
    "                                        'p2', 'p2_dog', 'p3', 'p3_dog']]\n",
    "# print(wrong_breeds.info())\n",
    "idx = list(wrong_breeds.index.values)\n",
    "\n",
    "# drop the missing dogs dataframe\n",
    "image_predictions_clean.drop(idx , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Choose one dog breed out of the three using the highest confidence and save it in a different column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = []  \n",
    "dog_breed = []\n",
    "real_breed = []\n",
    "\n",
    "#Find the confidence and dog breeds   \n",
    "for idx in image_predictions_clean.index:\n",
    "    if ((image_predictions_clean['p1_conf'][idx] > image_predictions_clean['p2_conf'][idx]) & (image_predictions_clean['p1_conf'][idx] > image_predictions_clean['p3_conf'][idx])) & (image_predictions_clean['p1_dog'][idx] == True):\n",
    "            confidence.append(image_predictions_clean['p1_conf'][idx])\n",
    "            dog_breed.append(image_predictions_clean['p1'][idx])\n",
    "    elif ((image_predictions_clean['p2_conf'][idx] > image_predictions_clean['p3_conf'][idx]) & (image_predictions_clean['p2_dog'][idx] == True)):\n",
    "            confidence.append(image_predictions_clean['p2_conf'][idx])\n",
    "            dog_breed.append(image_predictions_clean['p2'][idx])\n",
    "    elif (image_predictions_clean['p3_dog'][idx] == True ):\n",
    "            confidence.append(image_predictions_clean['p3_conf'][idx])\n",
    "            dog_breed.append(image_predictions_clean['p3'][idx])\n",
    "    else:\n",
    "        print('dont know the error')\n",
    "        \n",
    "#convert the lists to numpy arrays\n",
    "confidence = np.array(confidence)\n",
    "dog_breed = np.array(dog_breed)\n",
    "\n",
    "print(len(confidence))\n",
    "print(len(dog_breed))\n",
    "#print(len(errors))\n",
    "\n",
    "\n",
    "print(len(image_predictions_clean))\n",
    "\n",
    "#add these arrays to the dataframe as rows\n",
    "image_predictions_clean['confidence'] = confidence\n",
    "image_predictions_clean['dog_breed'] = dog_breed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_clean.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "We chose one column out of the three and saved the dog_breed column\n",
    "- Correct the *dog_breed* column to be categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dog_breeds\n",
    "dog_breed_classes = image_predictions_clean.loc[:, \"dog_breed\"].to_list()\n",
    "\n",
    "#remove any duplicates\n",
    "dog_breed_classes = list(dict.fromkeys(dog_breed_classes))\n",
    "dog_breed_classes = np.array(dog_breed_classes)\n",
    "type(dog_breed_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dog_breed_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_ver = pd.__version__.split(\".\")\n",
    "if (int(pd_ver[0]) > 0) or (int(pd_ver[1]) >= 21): # v0.21 or later\n",
    "    dclasses = pd.api.types.CategoricalDtype(ordered = False, categories = dog_breed_classes)\n",
    "    image_predictions_clean['dog_breed'] = image_predictions_clean['dog_breed'].astype(dclasses)\n",
    "else: # pre-v0.21\n",
    "    image_predictions_clean['dog_breed'] = image_predictions_clean['dog_breed'].astype('category', \n",
    "                                                                ordered = False, categories = dog_breed_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Drop *p1*, *p2*, *p3*, *p1_conf*, *p2_conf*, *p3_conf*, *p1_dog*, *p2_dog*, *p3_dog*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_clean.drop(['p1', 'p2', 'p3', 'p1_conf', 'p2_conf', 'p3_conf', 'p1_dog', 'p2_dog', 'p3_dog'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Drop *jpg_url* from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unncessary columns\n",
    "image_predictions_clean.drop(['jpg_url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tweet_json_df` Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Correct *tweet_id* datatype to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change datatypes\n",
    "tweet_json_clean['tweet_id'] = tweet_json_clean['tweet_id'].astype(int)\n",
    "tweet_json_clean['date_time'] = pd.to_datetime(tweet_json_clean['date_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define \n",
    "- Change `tweet_json_clean`'s *date_time* column name to *datetime*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the dataframe column\n",
    "tweet_json_clean = tweet_json_clean.rename(columns={\"date_time\":\"timestamp\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> After cleaning all the dataset individually,, we then have to combine all the datasets to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Join all the three tables to one. First combine `twitter_archive_clean` and `tweet_json_clean`. Then combine the results with `image_recognition_clean`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge twitter_archive_clean and tweet_json_clean\n",
    "df1 = pd.merge(twitter_archive_clean, tweet_json_clean, on = ['tweet_id', 'timestamp'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the new df with *image_recogition_clean*\n",
    "rate_dogs_df = pd.merge(df1, image_predictions_clean, on=['tweet_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_dogs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Drop any null rows in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with null values\n",
    "rate_dogs_df.dropna(axis=0, how = 'any', inplace = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_dogs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset as a csv file\n",
    "rate_dogs_df.to_csv('twitter_archive_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analyse'></a>\n",
    "## Analysis and Vizualization\n",
    "\n",
    "After assessing and cleaning our dataset, we can therefore go ahead and analyse it. Some of the question we want use to asses the dataset are:\n",
    "\n",
    "- What are most common breed rated by WeRateDogs? Top 10.\n",
    "- Is there a pair-wise relationship between retweets, favorites and the ratings given by the account?\n",
    "- What is the relationship between retweets and fevorites?\n",
    "- What is the relationship betweet ratings and retweets?\n",
    "- What is the relationship betweet ratings and favorites?\n",
    "- Is there a relationship between number of retweets, favorites and the ratings given by the account?\n",
    "- What is the most common ratings given by the twitter account?\n",
    "\n",
    "We are therefore going to do some exploratory visual analysis to try and answer these questions.\n",
    "\n",
    "### Q1:  What are most common dog breeds rated by WeRateDogs? Top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count function\n",
    "def counter(column):\n",
    "    \n",
    "    #join all the entries in the column to a single string\n",
    "    concat_str = rate_dogs_df[column].str.cat(sep='|')\n",
    "    \n",
    "    # Change the string to series\n",
    "    sep_str = pd.Series(concat_str.split(sep='|'))\n",
    "    \n",
    "    return sep_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the series in a dataframe \n",
    "dog_breed_df = counter('dog_breed').to_frame('dog_breed')\n",
    "\n",
    "#Get the unique values\n",
    "dog_breed_df = dog_breed_df['dog_breed'].value_counts().to_frame('dog_breed').reset_index()\n",
    "\n",
    "#rename columns\n",
    "dog_breed_df = dog_breed_df.rename(columns={'index':'dog_breed','dog_breed':'dog_num'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cut the first 10 dog breeds\n",
    "new_dog_breed = dog_breed_df.iloc[:10]\n",
    "\n",
    "#Set color for the bar graph\n",
    "base_color = sns.color_palette()[0]\n",
    "fig_dims = (20, 15)\n",
    "fig, ax = plt.subplots(figsize = fig_dims)\n",
    "\n",
    "#Plot the bargraph\n",
    "sns.barplot(data = new_dog_breed, x = 'dog_num', y = 'dog_breed', color = base_color, ax = ax);\n",
    "\n",
    "#Set the title and axes\n",
    "plt.title(\"Top 10 Most Common Dog Breed\", fontsize=16);\n",
    "plt.xlabel(\"Total Number Of Dogs\",fontsize=15);\n",
    "plt.ylabel(\"Dog Breeds\",fontsize= 15);\n",
    "plt.savefig('dbreed.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Is there a pair-wise relationship between retweets, likes, rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(rate_dogs_df, vars = ['rating_numerator', 'retweets', 'favorites'],\n",
    "            plot_kws = {'alpha' : 1/5},diag_kind = 'kde', height = 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: What is the relationship between retweets and favorites?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the scales\n",
    "fig_dims = (20, 15)\n",
    "fig, ax = plt.subplots(figsize = fig_dims)\n",
    "plt.xscale('log');\n",
    "plt.yscale('log');\n",
    "#plot \n",
    "plt.scatter(data = rate_dogs_df, x = 'favorites', y = 'retweets' );\n",
    "plt.title('Retweets vs Favorites ');\n",
    "plt.xlabel('Favorites (log)');\n",
    "plt.ylabel('Retweets (log)');\n",
    "plt.savefig('retwfav.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: What is the relationship betweet ratings and retweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the scales\n",
    "fig_dims = (20, 15)\n",
    "fig, ax = plt.subplots(figsize = fig_dims)\n",
    "plt.xscale('log');\n",
    "\n",
    "#plot \n",
    "sns.regplot(data = rate_dogs_df, x = 'retweets', y = 'rating_numerator', fit_reg = False, \n",
    "          x_jitter=0.2, y_jitter=0.1, scatter_kws = {'alpha': 1/3});\n",
    "\n",
    "plt.axhline(y = 9, color = 'r');\n",
    "plt.title('Retweets vs Ratings ');\n",
    "plt.xlabel('Retweets (log)');\n",
    "plt.ylabel('Ratings');\n",
    "plt.savefig('rr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: What is the relationship betweet ratings and favorites?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the scales\n",
    "fig_dims = (20, 15)\n",
    "fig, ax = plt.subplots(figsize = fig_dims)\n",
    "plt.xscale('log');\n",
    "\n",
    "#plot \n",
    "sns.regplot(data = rate_dogs_df, x = 'favorites', y = 'rating_numerator', fit_reg = False, \n",
    "          x_jitter=0.2, y_jitter=0.1, scatter_kws = {'alpha': 1/3});\n",
    "plt.axhline(y = 9, color = 'r');\n",
    "plt.title('Favorites vs Ratings ');\n",
    "plt.xlabel('Favorites (log)');\n",
    "plt.ylabel('Ratings');\n",
    "plt.savefig('rf.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4:Is there a relationship between number of retweets, likes and the ratings given by the account?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set dimension size\n",
    "fig_dims = (20, 15)\n",
    "fig, ax = plt.subplots(figsize = fig_dims)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim(10 , 1000000)\n",
    "plt.ylim(10, 1000000)\n",
    "\n",
    "#plot \n",
    "plt.scatter(data = rate_dogs_df, x = 'retweets', y = 'favorites', c = 'rating_numerator', cmap = 'viridis_r');\n",
    "plt.colorbar();\n",
    "plt.savefig('rrf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: What is the most common ratings given by the twitter account?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dims = (20, 15)\n",
    "fig, ax = plt.subplots(figsize = fig_dims)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.boxplot(y =rate_dogs_df['rating_numerator'])\n",
    "plt.savefig('b_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use histogram\n",
    "#set size\n",
    "fig_dims = (20, 15)\n",
    "fig, ax = plt.subplots(figsize = fig_dims)\n",
    "\n",
    "bins = np.arange(0, rate_dogs_df['rating_numerator'].max()+1, 1);\n",
    "plt.hist(data = rate_dogs_df, x = 'rating_numerator', bins = bins);\n",
    "plt.savefig('h_rating.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='insight'></a>\n",
    "## Conclusions\n",
    "\n",
    "After analysis of our cleaned dataset, the following conclutions were drawn:\n",
    "- Retweets and favorites have a strong positive correlation. This means that the as the number of retweets increase so does the number of favorites.\n",
    "- Dogs that have a high ratings tends to have more retwwets and favorites.\n",
    "- Highly rated dogs have a high number retweets.\n",
    "- Highly rated dogs have a high number favorites.\n",
    "- The rating with the highest number of dogs is 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "## Reference\n",
    "\n",
    "[Pandas Documentation](\"https://pandas.pydata.org/docs/user_guide/\")\n",
    "\n",
    "[Numpy Documentation](\"https://numpy.org/doc/stable/user/\")\n",
    "\n",
    "[Seaborn Documentation](\"https://seaborn.pydata.org/tutorial.html\")\n",
    "\n",
    "[Matplotlib Documentation](\"https://matplotlib.org/3.2.1/tutorials/index.html\")\n",
    "\n",
    "[Github](\"https://github.com/stephanderton/We-Rate-Dogs-Data-Wrangling-Project\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
